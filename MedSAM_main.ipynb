{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cpu\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Check system install\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())  # Should return True if GPU is detected\n",
    "\n",
    "# General Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm \n",
    "from torch.optim import Adam, SGD\n",
    "import monai\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from torch.utils.data import DataLoader\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import threshold, normalize\n",
    "from monai.losses import DiceLoss\n",
    "from statistics import mean\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import ExperimentAnalysis\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Class Imports\n",
    "# Reload modules so classes are reloaded every time\n",
    "import importlib\n",
    "import image_mask_dataset\n",
    "import model_evaluator\n",
    "\n",
    "from image_mask_dataset import ImageMaskDataset\n",
    "from model_evaluator import ModelEvaluator\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# MedSAM\n",
    "from transformers import SamModel, SamProcessor, SamConfig\n",
    "\n",
    "from segment_anything import sam_model_registry\n",
    "\n",
    "#from MedSAM.utils.demo import BboxPromptDemo\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather data for each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [00:00<00:00, 17366.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid image-mask pairs found: 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1703/1703 [00:00<00:00, 18209.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid image-mask pairs found: 1700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:00<00:00, 18212.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid image-mask pairs found: 154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(640, 640, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(image_mask_dataset)\n",
    "\n",
    "# Initialize the processor\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-large\")\n",
    "\n",
    "# Create dataset objects for each split\n",
    "dataset_path = \"Datasets/Dental project.v19i.coco-1\"\n",
    "\n",
    "test_dataset = ImageMaskDataset(dataset_path, \"test\", processor)\n",
    "train_dataset = ImageMaskDataset(dataset_path, \"train\", processor)\n",
    "valid_dataset = ImageMaskDataset(dataset_path, \"valid\", processor)\n",
    "\n",
    "\n",
    "\n",
    "test_dataset[0][\"pixel_values\"].shape\n",
    "\n",
    "# Test using a random image\n",
    "# test_dataset.show_image_mask(random.randint(0,len(test_dataset)-1))\n",
    "# train_dataset.show_image_mask(random.randint(0,len(train_dataset)-1))\n",
    "# valid_dataset.show_image_mask(random.randint(0,len(valid_dataset)-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise MedSAM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Models/medsam_vit_b.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m MedSAM_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModels/medsam_vit_b.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m medsam_model \u001b[38;5;241m=\u001b[39m \u001b[43msam_model_registry\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvit_b\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMedSAM_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m medsam_model \u001b[38;5;241m=\u001b[39m medsam_model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m medsam_model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\segment_anything\\build_sam.py:38\u001b[0m, in \u001b[0;36mbuild_sam_vit_b\u001b[1;34m(checkpoint)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_sam_vit_b\u001b[39m(checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_build_sam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_embed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_num_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_global_attn_indexes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\segment_anything\\build_sam.py:104\u001b[0m, in \u001b[0;36m_build_sam\u001b[1;34m(encoder_embed_dim, encoder_depth, encoder_num_heads, encoder_global_attn_indexes, checkpoint)\u001b[0m\n\u001b[0;32m    102\u001b[0m sam\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    105\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m    106\u001b[0m     sam\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Models/medsam_vit_b.pth'"
     ]
    }
   ],
   "source": [
    "MedSAM_checkpoint = \"Models/medsam_vit_b.pth\"\n",
    "\n",
    "medsam_model = sam_model_registry['vit_b'](checkpoint=MedSAM_checkpoint)\n",
    "medsam_model = medsam_model.to(\"cuda\")\n",
    "\n",
    "medsam_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Prompt Inference Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select random image\n",
    "image_idx = random.randint(0, len(test_dataset)-1)\n",
    "image = test_dataset.image_mask_pairs[image_idx][0]\n",
    "\n",
    "# Display image\n",
    "%matplotlib inline\n",
    "\n",
    "test_dataset.show_image_mask(image_idx)\n",
    "\n",
    "# Segment image\n",
    "%matplotlib widget\n",
    "bbox_prompt_demo = BboxPromptDemo(medsam_model)\n",
    "bbox_prompt_demo.show(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "    \n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importlib.reload(model_evaluator)\n",
    "evaluator = ModelEvaluator(medsam_model, processor, test_dataset)\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Get correct preprocessing\n",
    "test_dataset.return_as_medsam = True\n",
    "test_dataset.resize_mask = False\n",
    "\n",
    "# Load random image\n",
    "image_idx = random.randint(0, len(test_dataset)-1)\n",
    "\n",
    "\n",
    "# Get tensors\n",
    "img_np, box_np, gt_masks, bounding_boxes = test_dataset[image_idx].values()\n",
    "\n",
    "# Get original image\n",
    "test_dataset.return_as_medsam = False\n",
    "img_original = test_dataset[image_idx][\"pixel_values\"]\n",
    "W, H, _ = img_original.shape\n",
    "\n",
    "# Show image\n",
    "test_dataset.show_image_mask(image_idx)\n",
    "\n",
    "# image embedding\n",
    "with torch.no_grad():\n",
    "    image_embedding = medsam_model.image_encoder(img_np)\n",
    "\n",
    "\n",
    "# Run inference for all boxes in a batch\n",
    "with torch.no_grad():\n",
    "    seg_masks = evaluator.medsam_inference(image_embedding, box_np, H, W)  # List of 5 masks\n",
    "\n",
    "# Plot results\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "\n",
    "if len(seg_masks.shape) == 2:\n",
    "    seg_masks = [seg_masks]\n",
    "\n",
    "\n",
    "# Original image with bounding boxes\n",
    "ax[0].imshow(img_original)\n",
    "for box in bounding_boxes:\n",
    "    show_box(box, ax[0])\n",
    "ax[0].set_title(\"Input Image and Bounding Boxes\")\n",
    "\n",
    "# Image with segmentation masks\n",
    "ax[1].imshow(img_original)\n",
    "for box, mask in zip(bounding_boxes, seg_masks):  # Iterate over all boxes and their masks\n",
    "    show_mask(mask, ax[1]) # random_colour = True\n",
    "    show_box(box, ax[1])\n",
    "ax[1].set_title(\"Base MedSAM Segmentation\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MedSAM Base Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluator.evaluate_medsam_base_model()\n",
    "evaluator.print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set and verify dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure images are returned as preprocessed tensors of the right size\n",
    "test_dataset.preprocess_for_fine_tuning  = True\n",
    "train_dataset.preprocess_for_fine_tuning = True\n",
    "valid_dataset.preprocess_for_fine_tuning = True\n",
    "\n",
    "test_dataset.resize_mask  = True\n",
    "train_dataset.resize_mask = True\n",
    "valid_dataset.resize_mask = True\n",
    "\n",
    "test_dataset.return_individual_objects = True\n",
    "train_dataset.return_individual_objects = True\n",
    "valid_dataset.return_individual_objects = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Veryify item sizes\n",
    "example = train_dataset[0]\n",
    "\n",
    "for k,v in example.items():\n",
    "  print(f\"{k:<25} Shape: {str(v.shape):<30} Dtype: {v.dtype}\")\n",
    "\n",
    "train_dataset.show_image_mask(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2  , shuffle=True, drop_last=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset,  batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Verify batch item sizes\n",
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  print(f\"{k:<25} Shape: {str(v.shape):<30} Dtype: {v.dtype}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get image from batch\n",
    "image = batch[\"pixel_values\"][0].detach().cpu().numpy().transpose(1, 2, 0)  # Convert to HxWxC format\n",
    "\n",
    "# Convert to HxWxC format (just adding a channel dimension if needed)\n",
    "ground_truth = batch[\"obj_ground_truth_masks\"][0][0].detach().cpu().numpy()  # Convert to numpy (H, W)\n",
    "ground_truth = np.expand_dims(ground_truth, axis=-1)  # Add a channel dimension (H, W, 1)\n",
    "\n",
    "print(\"Image Shape:\", image.shape)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(24, 8))\n",
    "\n",
    "\n",
    "\n",
    "ax1.imshow(image)  # Show the image\n",
    "# Plot predicted boxes\n",
    "first_box = True\n",
    "for box in batch[\"input_boxes\"][0]:\n",
    "    rect = patches.Rectangle(\n",
    "        (box[0], box[1]),  # x, y (top-left corner)\n",
    "        box[2] - box[0],  # width\n",
    "        box[3] - box[1],  # height\n",
    "        linewidth=2,\n",
    "        edgecolor='red',\n",
    "        facecolor='none',\n",
    "        label='Predicted Box'\n",
    "    )\n",
    "\n",
    "    if first_box:\n",
    "       first_box = False\n",
    "       rect.set_edgecolor(\"green\")\n",
    "\n",
    "    ax1.add_patch(rect)\n",
    "    \n",
    "\n",
    "ax1.set_title(f\"Example Input\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Ground Truth Shape\", ground_truth.shape)\n",
    "\n",
    "ax2.imshow(ground_truth, cmap='gray')  # Show the second image\n",
    "# Plot predicted boxes for the second image\n",
    "box = batch[\"input_boxes\"][0][0]\n",
    "box = (box / torch.tensor([1024,1024,1024,1024], device=\"cpu\")) * 256\n",
    "rect = patches.Rectangle(\n",
    "    (box[0], box[1]),  # x, y (top-left corner)\n",
    "    box[2] - box[0],  # width\n",
    "    box[3] - box[1],  # height\n",
    "    linewidth=2,\n",
    "    edgecolor='green',\n",
    "    facecolor='none',\n",
    "    label='Predicted Box'\n",
    ")\n",
    "    \n",
    "ax2.add_patch(rect)\n",
    "ax2.set_title(f\"Example GT (Loss)\")\n",
    "\n",
    "\n",
    "ax3.imshow(batch[\"ground_truth_mask\"][0], cmap=\"gray\")\n",
    "ax3.set_title(f\"GT Mask\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning (Ray Tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Remote Actor for Valid Dataloader\n",
    " __**NOTE: This cell is where the the outputs from the tuning loop are output**__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a Ray actor to distribute validation data across workers\n",
    "@ray.remote\n",
    "class DataLoaderActor:\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "        self.iterator = iter(self.dataloader)\n",
    "\n",
    "    def get_batch(self):\n",
    "        try:\n",
    "            return next(self.iterator)\n",
    "        except StopIteration:\n",
    "            self.iterator = iter(self.dataloader)  # Reset iterator if exhausted\n",
    "            return next(self.iterator)\n",
    "\n",
    "    def get_length(self):\n",
    "        return len(self.dataloader)\n",
    "\n",
    "\n",
    "\n",
    "# Wrap the DataLoader inside a Ray actor\n",
    "valid_dataloader_actor = DataLoaderActor.remote(valid_dataloader)\n",
    "\n",
    "\n",
    "# Define the custom short directory name function (Avoids window's restriction of paths being < 260 chars)\n",
    "def short_dirname(trial):\n",
    "    return \"trial_\" + str(trial.trial_id[:13])  # Shorten to 8 characters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid_boxes(input_boxes, obj_ground_truth_masks):\n",
    "    # Create a mask to identify input_boxes that are exactly [0, 0, 0, 0]\n",
    "    valid_mask = ~(input_boxes == torch.tensor([0, 0, 0, 0], dtype=input_boxes.dtype, device=input_boxes.device)).all(dim=-1)\n",
    "\n",
    "    # Filter input boxes and corresponding masks for each image in the batch (batch size = 1)\n",
    "    filtered_input_boxes = input_boxes[valid_mask]\n",
    "    filtered_obj_ground_truth_masks = obj_ground_truth_masks[valid_mask]\n",
    "\n",
    "    # Return filtered input boxes and ground truth masks, maintaining batch size of 1\n",
    "    return filtered_input_boxes.unsqueeze(0), filtered_obj_ground_truth_masks.unsqueeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning The Optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate model with different optimizer settings\n",
    "def tune_optimizer(config, dataloader_actor):\n",
    "\n",
    "\n",
    "    # Clear CUDA memory at the start of each call\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Initialize model\n",
    "    medsam_model = SamModel.from_pretrained(\"flaviagiammarino/medsam-vit-base\")\n",
    "\n",
    "    # Freeze encoder layers\n",
    "    for name, param in medsam_model.named_parameters():\n",
    "        if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "            param.requires_grad_(False)\n",
    "    \n",
    "    medsam_model.to(\"cuda\")\n",
    "\n",
    "    # Define optimizer based on config\n",
    "    if config[\"optimizer\"] == \"adam\":\n",
    "        optimizer = Adam(\n",
    "            params=medsam_model.mask_decoder.parameters(),\n",
    "            lr=config[\"lr\"],      \n",
    "            weight_decay=config[\"weight_decay\"]   \n",
    "        )\n",
    "    elif config[\"optimizer\"] == \"sgd\":\n",
    "        optimizer = SGD(\n",
    "            params=medsam_model.mask_decoder.parameters(),\n",
    "            lr=config[\"lr\"],\n",
    "            momentum=config[\"momentum\"],\n",
    "            weight_decay=config[\"weight_decay\"]\n",
    "        )\n",
    "\n",
    "    # Use Focal Loss with predefined sensible parameters\n",
    "    loss_fn = monai.losses.FocalLoss(\n",
    "        gamma=2.0,  # Common default for Focal Loss\n",
    "        reduction=\"mean\",\n",
    "        include_background=True\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Train for one epoch with the current configuration\n",
    "    epoch_losses = []\n",
    "    for _ in tqdm(range(ray.get(dataloader_actor.get_length.remote()))):\n",
    "        batch = ray.get(dataloader_actor.get_batch.remote())  # Fetch batch using Ray actor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Get batch values for inference\n",
    "        pixel_values = batch[\"pixel_values\"].to(\"cuda\")\n",
    "        input_boxes = batch[\"input_boxes\"].to(\"cuda\")\n",
    "        obj_ground_truth_masks = batch[\"obj_ground_truth_masks\"].float().to(\"cuda\").squeeze(1)  # Remove extra singleton dimension\n",
    "\n",
    "\n",
    "\n",
    "        # To ge the mean of each individual image in the batch\n",
    "        batch_loss_values = []\n",
    "\n",
    "\n",
    "        # Loops through each image in the batch, removes the padding on input_boxes and obj_ground_truth_masks to ensure the loss isn't miscalculated by using empty inputs (because that happens??)\n",
    "        # We take the mean of the batch still (using batch_loss_values) to ensure a smoother gradient by avoiding the noise from using individual images\n",
    "        for image, input_box, obj_mask in zip(pixel_values, input_boxes, obj_ground_truth_masks):\n",
    "\n",
    "            # Remove the padding from these batch values\n",
    "            input_box, obj_mask = remove_invalid_boxes(input_box, obj_mask)\n",
    "\n",
    "\n",
    "            # If the input somehow has no object masks we can skip\n",
    "            if input_box.shape[1] > 0:\n",
    "\n",
    "                # forward pass\n",
    "                outputs = medsam_model(\n",
    "                    pixel_values=image.unsqueeze(0), # Add batch dimension back\n",
    "                    input_boxes=input_box,\n",
    "                    multimask_output=False)\n",
    "\n",
    "\n",
    "                # Get predicted masks and ground truth masks\n",
    "                predicted_masks = outputs.pred_masks.squeeze(2)  # Remove extra singleton dimension from predicted masks (shape: [1, 20, 256, 256])\n",
    "            \n",
    "                # Convert object ground truth masks to binary to pass into MONAI loss function\n",
    "                obj_mask = (obj_mask > 0).float()\n",
    "\n",
    "                # Ensure the predicted and ground truth masks have the same shape\n",
    "                #print(\"\\n\\nPredicted Mask shape: \",predicted_masks.shape)\n",
    "                #print(\"obj gt shape: \", obj_mask.shape)\n",
    "\n",
    "                # Convert logits to probabilities\n",
    "                predicted_masks = torch.sigmoid(predicted_masks) \n",
    "\n",
    "                # Calculate loss using defined loss function\n",
    "                batch_loss_values.append(loss_fn(predicted_masks, obj_mask))\n",
    "                print(\"Object Loss: \",batch_loss_values[-1])\n",
    "\n",
    "\n",
    "\n",
    "                # Show predictions\n",
    "                # with torch.no_grad():\n",
    "                #         first_pred_mask = torch.sigmoid(predicted_masks[0, 0]).cpu().numpy()  # Convert to numpy for plotting\n",
    "                #         first_gt_mask = obj_mask[0, 0].cpu().numpy()\n",
    "\n",
    "                #         fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "                #         ax[0].imshow(first_pred_mask, cmap=\"gray\")\n",
    "                #         ax[0].set_title(\"Predicted Mask\")\n",
    "\n",
    "                #         ax[1].imshow(first_gt_mask, cmap=\"gray\")\n",
    "                #         ax[1].set_title(\"Ground Truth Mask\")\n",
    "\n",
    "                #         plt.show()\n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "                # Debug print to catch the missing masks\n",
    "                print(\"No masks found for:\", input_box)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Calculate the mean of the batch and convert into a torch tensor for backpropagation\n",
    "        loss = torch.stack(batch_loss_values).mean() if batch_loss_values else None\n",
    "        print(\"Batch Loss: \", loss)\n",
    "\n",
    "\n",
    "        # If no masks found continue without processing batch\n",
    "        if not loss:\n",
    "            continue\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Compute and report mean loss for the epoch\n",
    "    mean_epoch_loss = mean(epoch_losses)\n",
    "\n",
    "\n",
    "    # Store metrics for these hyperparameters\n",
    "    metrics = {\n",
    "            \"loss\": mean_epoch_loss\n",
    "        }\n",
    "    \n",
    "    tune.report(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the search space for optimizer tuning\n",
    "search_space_optimizer = {\n",
    "    \"optimizer\": tune.choice([\"adam\", \"sgd\"]),\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-3), \n",
    "    \"weight_decay\": tune.uniform(0, 1e-3), \n",
    "    \"momentum\": tune.uniform(0, 0.99),  # SGD-specific momentum\n",
    "}\n",
    "\n",
    "# Get the absolute path for 'ray_results'\n",
    "storage_path = os.path.abspath(\"Ray Results/Optimizer\")\n",
    "\n",
    "# Run Ray Tune with the absolute path\n",
    "optimizer_result = tune.run(\n",
    "    lambda config: tune_optimizer(config, valid_dataloader_actor),  # Lambda function to pass dataloader_actor\n",
    "    config=search_space_optimizer,  # The search space\n",
    "    num_samples=1,  # Number of trials to run\n",
    "    trial_dirname_creator=short_dirname,  # Custom trial log directory name\n",
    "    resources_per_trial={\"gpu\": 1},  # Ensure 1 GPU per trial\n",
    "    storage_path=storage_path  # Use the absolute path for storage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the best trial based on the lowest loss\n",
    "best_trial = optimizer_result.get_best_trial(metric=\"loss\", mode=\"min\")\n",
    "\n",
    "# Output the best hyperparameters\n",
    "best_optimizer = best_trial.config[\"optimizer\"]\n",
    "best_momentum  = best_trial.config[\"momentum\"] if best_optimizer == \"sgd\" else None  # Use momentum only for SGD\n",
    "\n",
    "# Print results\n",
    "print(f\"Best Loss Value: {best_trial.last_result[\"loss\"]}\")\n",
    "print(f\"Best Optimizer: {best_optimizer}\")\n",
    "print(f\"Best Learning Rate: {best_trial.config[\"lr\"]:.6f}\")\n",
    "print(f\"Best Weight Decay: {best_trial.config[\"weight_decay\"]:.6f}\")\n",
    "if best_momentum is not None:\n",
    "    print(f\"Best Momentum: {best_momentum:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best saved params for optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ray.tune import ExperimentAnalysis\n",
    "\n",
    "# Get the absolute path for 'Ray Results/Optimizer'\n",
    "results_dir = os.path.abspath(\"Ray Results/Optimizer\")\n",
    "\n",
    "# Optimizer types to filter\n",
    "optimizers = [\"adam\", \"sgd\"]\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(results_dir):\n",
    "    print(f\"Directory {results_dir} does not exist.\")\n",
    "else:\n",
    "    # Loop through each optimizer\n",
    "    for optimizer_fn in optimizers:\n",
    "        \n",
    "        # List all files (subdirectories) in the results directory\n",
    "        for subdir in os.listdir(results_dir):\n",
    "            subdir_path = os.path.join(results_dir, subdir)\n",
    "            \n",
    "            # Only proceed if it's a directory (skip files)\n",
    "            if os.path.isdir(subdir_path):\n",
    "                try:\n",
    "                    # Load the ExperimentAnalysis object for this subdir\n",
    "                    analysis = ExperimentAnalysis(subdir_path)\n",
    "                    \n",
    "                    # Retrieve all trials and filter them by the current optimizer function\n",
    "                    trials = analysis.trials  # Get all trials\n",
    "                    filtered_trials = [trial for trial in trials if trial.config[\"optimizer\"] == optimizer_fn]\n",
    "                    \n",
    "                    print(f\"\\nBest Parameters for {optimizer_fn.capitalize()} Optimizer:\")\n",
    "                    \n",
    "                    if not filtered_trials:\n",
    "                        print(f\"        No valid trial found for {optimizer_fn} optimizer.\")\n",
    "                    else:\n",
    "                        # Get the best trial (the one with the minimum loss)\n",
    "                        best_trial = min(filtered_trials, key=lambda t: t.last_result[\"loss\"])\n",
    "                        \n",
    "                        # Retrieve the best configuration and loss value\n",
    "                        best_optimizer_config = best_trial.config  # Best hyperparameters\n",
    "                        best_loss = best_trial.last_result[\"loss\"]  # Best loss value\n",
    "                        \n",
    "                        # Print results for the current optimizer and subdirectory\n",
    "                        print(f\"        Config: {best_optimizer_config}\")\n",
    "                        print(f\"        Best Loss Value: {best_loss:.6f}\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {subdir_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning The Loss Funtions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to train and evaluate model with different loss function settings\n",
    "def tune_loss(config, dataloader_actor):\n",
    "\n",
    "\n",
    "    # Clear CUDA memory at the start of each call\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Initialize model\n",
    "    medsam_model = SamModel.from_pretrained(\"flaviagiammarino/medsam-vit-base\")\n",
    "\n",
    "    # Freeze encoder layers\n",
    "    for name, param in medsam_model.named_parameters():\n",
    "        if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "            param.requires_grad_(False)\n",
    "    \n",
    "    medsam_model.to(\"cuda\")\n",
    "\n",
    "    # Baseline Optimizer\n",
    "    optimizer = Adam(\n",
    "        params=medsam_model.mask_decoder.parameters(),\n",
    "        lr=0.0001,      \n",
    "        weight_decay=0   \n",
    "    )\n",
    "\n",
    "    # Choose loss functions\n",
    "    if config[\"loss\"] == \"dice\":\n",
    "        loss_fn = monai.losses.DiceLoss(\n",
    "            squared_pred=config[\"squared_pred\"],\n",
    "            reduction=\"mean\",\n",
    "            include_background=config[\"include_background\"]\n",
    "        )\n",
    "    elif config[\"loss\"] == \"focal\":\n",
    "        loss_fn = monai.losses.FocalLoss(\n",
    "            gamma=config[\"gamma\"],\n",
    "            reduction=\"mean\",\n",
    "            include_background=config[\"include_background\"]\n",
    "        )\n",
    "    elif config[\"loss\"] == \"tversky\":\n",
    "        loss_fn = monai.losses.TverskyLoss(\n",
    "            alpha=config[\"alpha\"],\n",
    "            beta=config[\"beta\"],\n",
    "            reduction=\"mean\",\n",
    "            include_background=config[\"include_background\"]\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    # Train for one epoch with the current configuration\n",
    "    epoch_losses = []\n",
    "    for _ in tqdm(range(ray.get(dataloader_actor.get_length.remote()))):\n",
    "        batch = ray.get(dataloader_actor.get_batch.remote())  # Fetch batch using Ray actor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Get batch values for inference\n",
    "        pixel_values = batch[\"pixel_values\"].to(\"cuda\")\n",
    "        input_boxes = batch[\"input_boxes\"].to(\"cuda\")\n",
    "        obj_ground_truth_masks = batch[\"obj_ground_truth_masks\"].float().to(\"cuda\").squeeze(1)  # Remove extra singleton dimension\n",
    "\n",
    "\n",
    "\n",
    "        # To ge the mean of each individual image in the batch\n",
    "        batch_loss_values = []\n",
    "\n",
    "\n",
    "        # Loops through each image in the batch, removes the padding on input_boxes and obj_ground_truth_masks to ensure the loss isn't miscalculated by using empty inputs (because that happens??)\n",
    "        # We take the mean of the batch still (using batch_loss_values) to ensure a smoother gradient by avoiding the noise from using individual images\n",
    "        for image, input_box, obj_mask in zip(pixel_values, input_boxes, obj_ground_truth_masks):\n",
    "\n",
    "            # Remove the padding from these batch values\n",
    "            input_box, obj_mask = remove_invalid_boxes(input_box, obj_mask)\n",
    "\n",
    "\n",
    "            # If the input somehow has no object masks we can skip\n",
    "            if input_box.shape[1] > 0:\n",
    "\n",
    "                # forward pass\n",
    "                outputs = medsam_model(\n",
    "                    pixel_values=image.unsqueeze(0), # Add batch dimension back\n",
    "                    input_boxes=input_box,\n",
    "                    multimask_output=False)\n",
    "\n",
    "\n",
    "                # Get predicted masks and ground truth masks\n",
    "                predicted_masks = outputs.pred_masks.squeeze(2)  # Remove extra singleton dimension from predicted masks (shape: [1, 20, 256, 256])\n",
    "            \n",
    "                # Convert object ground truth masks to binary to pass into MONAI loss function\n",
    "                obj_mask = (obj_mask > 0).float()\n",
    "\n",
    "                # Ensure the predicted and ground truth masks have the same shape\n",
    "                #print(\"\\n\\nPredicted Mask shape: \",predicted_masks.shape)\n",
    "                #print(\"obj gt shape: \", obj_mask.shape)\n",
    "\n",
    "                # Convert logits to probabilities\n",
    "                predicted_masks = torch.sigmoid(predicted_masks) \n",
    "\n",
    "                # Calculate loss using defined loss function\n",
    "                batch_loss_values.append(loss_fn(predicted_masks, obj_mask))\n",
    "                print(\"Object Loss: \",batch_loss_values[-1])\n",
    "\n",
    "\n",
    "\n",
    "                # Show predictions\n",
    "                # with torch.no_grad():\n",
    "                #         first_pred_mask = torch.sigmoid(predicted_masks[0, 0]).cpu().numpy()  # Convert to numpy for plotting\n",
    "                #         first_gt_mask = obj_mask[0, 0].cpu().numpy()\n",
    "\n",
    "                #         fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "                #         ax[0].imshow(first_pred_mask, cmap=\"gray\")\n",
    "                #         ax[0].set_title(\"Predicted Mask\")\n",
    "\n",
    "                #         ax[1].imshow(first_gt_mask, cmap=\"gray\")\n",
    "                #         ax[1].set_title(\"Ground Truth Mask\")\n",
    "\n",
    "                #         plt.show()\n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "                # Debug print to catch the missing masks\n",
    "                print(\"No masks found for:\", input_box)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Calculate the mean of the batch and convert into a torch tensor for backpropagation\n",
    "        loss = torch.stack(batch_loss_values).mean() if batch_loss_values else None\n",
    "        print(\"Batch Loss: \", loss)\n",
    "\n",
    "        # If no masks found continue without processing batch\n",
    "        if not loss:\n",
    "            continue\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # optimize\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Compute and report mean loss for the epoch\n",
    "    mean_epoch_loss = mean(epoch_losses)\n",
    "\n",
    "\n",
    "    # Store metrics for these hyperparameters\n",
    "    metrics = {\n",
    "            \"loss\": mean_epoch_loss\n",
    "        }\n",
    "    \n",
    "    tune.report(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the search space for loss function tuning\n",
    "search_space_loss = {\n",
    "    \"loss\":               tune.choice([\"dice\", \"focal\", \"tversky\"]),\n",
    "    \"include_background\": tune.choice([False]),    # Maybe add true, but I think its too imbalanced to use (binary imbalance)\n",
    "    \"squared_pred\":       tune.choice([True, False]),  # Only for Dice loss\n",
    "    \"gamma\":              tune.uniform(1.0, 5.0),  # Only for Focal loss (A higher value of gamma can be better for imbalanced classes (e.g. 3-5))\n",
    "    \"alpha\":              tune.uniform(0.3, 0.7),  # Only for Tversky loss (false positives)\n",
    "    \"beta\":               tune.uniform(0.3, 0.8)    # Only for Tversky loss (false negatives)\n",
    "}\n",
    "\n",
    "\n",
    "# Get the absolute path for 'ray_results'\n",
    "storage_path = os.path.abspath(\"Ray Results/Loss\")\n",
    "\n",
    "# Run Ray Tune with the absolute path\n",
    "loss_result = tune.run(\n",
    "    lambda config: tune_loss(config, valid_dataloader_actor),  # Lambda function to pass dataloader_actor\n",
    "    config=search_space_loss,  # The search space\n",
    "    num_samples=1,  # Number of trials to run\n",
    "    trial_dirname_creator=short_dirname,  # Custom trial log directory name\n",
    "    resources_per_trial={\"cpu\": 8, \"gpu\": 1},  # Ensure 1 GPU per trial\n",
    "    storage_path=storage_path  # Use the absolute path for storage\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = loss_result.get_best_trial(metric=\"loss\", mode=\"min\")\n",
    "\n",
    "print(\"Best Loss Value:\", best_trial.last_result[\"loss\"])\n",
    "print(\"Best Loss Config:\", best_trial.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best params for loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the absolute path for 'Ray Results/Loss'\n",
    "results_dir = os.path.abspath(\"Ray Results/Loss\")\n",
    "\n",
    "# Loss function types to filter\n",
    "loss_functions = [\"dice\", \"focal\", \"tversky\"]\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(results_dir):\n",
    "    print(f\"Directory {results_dir} does not exist.\")\n",
    "else:\n",
    "    # Loop through each loss function\n",
    "    for loss_fn in loss_functions:\n",
    "        \n",
    "        # List all files (subdirectories) in the results directory\n",
    "        for subdir in os.listdir(results_dir):\n",
    "            subdir_path = os.path.join(results_dir, subdir)\n",
    "            \n",
    "            # Only proceed if it's a directory (skip files)\n",
    "            if os.path.isdir(subdir_path):\n",
    "                try:\n",
    "                    # Load the ExperimentAnalysis object for this subdir\n",
    "                    analysis = ExperimentAnalysis(subdir_path)\n",
    "                    \n",
    "                    # Retrieve all trials and filter them by the current loss function\n",
    "                    trials = analysis.trials  # Get all trials\n",
    "                    filtered_trials = [trial for trial in trials if trial.config[\"loss\"] == loss_fn]\n",
    "\n",
    "                    print(f\"\\nBest Parameters for {loss_fn.capitalize()} Loss:\")\n",
    "                \n",
    "                    if not filtered_trials:\n",
    "                        print(f\"        No valid trial found for {loss_fn} loss.\")\n",
    "                    else:\n",
    "                        # Get the best trial (the one with the minimum loss)\n",
    "                        best_trial = min(filtered_trials, key=lambda t: t.last_result[\"loss\"])\n",
    "                        \n",
    "                        # Retrieve the best configuration and loss value\n",
    "                        best_loss_config = best_trial.config  # Best hyperparameters\n",
    "                        best_loss = best_trial.last_result[\"loss\"]  # Best loss value\n",
    "                        \n",
    "                        # Print results for the current loss function and subdirectory\n",
    "                        print(f\"        Config: {best_loss_config}\")\n",
    "                        print(f\"        Best Loss Value: {best_loss:.6f}\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {subdir_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning MedSAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Hugging Face SamProcessor (MedSAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove previous models from memory\n",
    "medsam_model = None\n",
    "del medsam_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# load the pretrained weights for finetuning\n",
    "medsam_model = SamModel.from_pretrained(\"flaviagiammarino/medsam-vit-base\")\n",
    "\n",
    "# make sure we only compute gradients for mask decoder\n",
    "for name, param in medsam_model.named_parameters():\n",
    "  if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "  \n",
    "device = \"cuda\"\n",
    "medsam_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set optimisers (These will be used for hyperparameter tuning later)\n",
    "\n",
    "# Initialize the optimizer and the loss function (May need to play with lower learning rates to avoid changing base model too much)\n",
    "adam_optimizer = Adam(medsam_model.mask_decoder.parameters(), lr=0.00001, weight_decay=0)\n",
    "\n",
    "sgd_optimizer  = SGD(medsam_model.mask_decoder.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "\n",
    "\n",
    "# Set loss functions\n",
    "\n",
    "# Define the DiceCELoss\n",
    "dice_loss = monai.losses.DiceLoss(\n",
    "    squared_pred=True,       # square predictions for  dice calculation (peanalises false positives)\n",
    "    reduction='mean',        # how losses are aggregated (mean, sum, or none)\n",
    "    include_background = False\n",
    ")\n",
    "\n",
    "\n",
    "# Define the Focal Loss (good for imbalanced pixel coverage)\n",
    "focal_loss = monai.losses.FocalLoss(\n",
    "    gamma=2.0,             # Focusing (higher = more focus on hard examples)\n",
    "    reduction='mean',       # How losses are aggregated (mean, sum, or none)\n",
    "    include_background = False\n",
    ")\n",
    "\n",
    "\n",
    "# Define Tversky Loss (false pos not that imporant, but false neg is bad)\n",
    "tversky_loss = monai.losses.TverskyLoss(\n",
    "    alpha=0.7,              # increase for less false positives\n",
    "    beta=0.3,               # increase for less false negatives\n",
    "    reduction='mean',       # how losses are aggregated (mean, sum, or none)\n",
    "    include_background = False\n",
    ")\n",
    "\n",
    "\n",
    "# Maybe also try focal or dicefocal loss?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid_boxes(input_boxes, obj_ground_truth_masks):\n",
    "    # Create a mask to identify input_boxes that are exactly [0, 0, 0, 0]\n",
    "    valid_mask = ~(input_boxes == torch.tensor([0, 0, 0, 0], dtype=input_boxes.dtype, device=input_boxes.device)).all(dim=-1)\n",
    "\n",
    "    # Filter input boxes and corresponding masks for each image in the batch (batch size = 1)\n",
    "    filtered_input_boxes = input_boxes[valid_mask]\n",
    "    filtered_obj_ground_truth_masks = obj_ground_truth_masks[valid_mask]\n",
    "\n",
    "    # Return filtered input boxes and ground truth masks, maintaining batch size of 1\n",
    "    return filtered_input_boxes.unsqueeze(0), filtered_obj_ground_truth_masks.unsqueeze(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "#Training loop\n",
    "num_epochs = 1\n",
    "batch_losses = []  # Store loss for each batch\n",
    "\n",
    "#medsam_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for batch in tqdm(train_dataloader):\n",
    "\n",
    "\n",
    "\n",
    "        # print(batch.keys())\n",
    "\n",
    "\n",
    "        # Get batch values for inference\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        input_boxes = batch[\"input_boxes\"].to(device)\n",
    "        obj_ground_truth_masks = batch[\"obj_ground_truth_masks\"].float().to(device).squeeze(1)  # Remove extra singleton dimension\n",
    "\n",
    "\n",
    "        # To ge the mean of each individual image in the batch\n",
    "        batch_loss_values = []\n",
    "\n",
    "\n",
    "        # Loops through each image in the batch, removes the padding on input_boxes and obj_ground_truth_masks to ensure the loss isn't miscalculated by using empty inputs (because that happens??)\n",
    "        # We take the mean of the batch still (using batch_loss_values) to ensure a smoother gradient by avoiding the noise from using individual images\n",
    "\n",
    "        \n",
    "\n",
    "        for image, input_box, obj_mask in zip(pixel_values, input_boxes, obj_ground_truth_masks):\n",
    "\n",
    "            # Remove the padding from these batch values\n",
    "            input_box, obj_mask = remove_invalid_boxes(input_box, obj_mask)\n",
    "\n",
    "\n",
    "            # If the input somehow has no object masks we can skip\n",
    "            if input_box.shape[1] > 0:\n",
    "\n",
    "                # forward pass\n",
    "                outputs = medsam_model(\n",
    "                    pixel_values=image.unsqueeze(0), # Add batch dimension back\n",
    "                    input_boxes=input_box,\n",
    "                    multimask_output=False)\n",
    "\n",
    "\n",
    "                # Get predicted masks and ground truth masks\n",
    "                predicted_masks = outputs.pred_masks.squeeze(2)  # Remove extra singleton dimension from predicted masks (shape: [1, 20, 256, 256])\n",
    "            \n",
    "                # Convert object ground truth masks to binary to pass into MONAI loss function\n",
    "                obj_mask = (obj_mask > 0).float()\n",
    "\n",
    "                # Ensure the predicted and ground truth masks have the same shape\n",
    "                #print(\"\\n\\nPredicted Mask shape: \",predicted_masks.shape)\n",
    "                #print(\"obj gt shape: \", obj_mask.shape)\n",
    "\n",
    "                # Convert logits to probabilities\n",
    "                predicted_masks = torch.sigmoid(predicted_masks) \n",
    "\n",
    "\n",
    "                # Calculate loss using defined loss function\n",
    "                batch_loss_values.append(dice_loss(predicted_masks, obj_mask))\n",
    "                print(\"Object Loss: \",batch_loss_values[-1])\n",
    "\n",
    "\n",
    "\n",
    "                # Show predictions\n",
    "                # with torch.no_grad():\n",
    "                #         first_pred_mask = torch.sigmoid(predicted_masks[0, 0]).cpu().numpy()  # Convert to numpy for plotting\n",
    "                #         first_gt_mask = obj_mask[0, 0].cpu().numpy()\n",
    "\n",
    "                #         fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "                #         ax[0].imshow(first_pred_mask, cmap=\"gray\")\n",
    "                #         ax[0].set_title(\"Predicted Mask\")\n",
    "\n",
    "                #         ax[1].imshow(first_gt_mask, cmap=\"gray\")\n",
    "                #         ax[1].set_title(\"Ground Truth Mask\")\n",
    "\n",
    "                #         plt.show()\n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "                # Debug print to catch the missing masks\n",
    "                print(\"No masks found for:\", input_box)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                        plt.imshow(image.cpu().numpy().squeeze().transpose(1,2,0))\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Calculate the mean of the batch and convert into a torch tensor for backpropagation\n",
    "        loss = torch.stack(batch_loss_values).mean() if batch_loss_values else None\n",
    "        print(\"\\nBatch Loss: \", loss)\n",
    "\n",
    "\n",
    "        # If no masks found continue without processing batch\n",
    "        if not loss:\n",
    "            continue\n",
    "\n",
    "        # backward pass\n",
    "        adam_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # optimize\n",
    "        adam_optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "        # Store batch loss\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # NOTE: sigmoid and softmax have the same output range here indicating that the SAM and MedSAM models only predict one class by default. This might \n",
    "        # Calculate the max and min values for both predicted and ground truth masks\n",
    "        # pred_min_value = F.softmax(predicted_masks).min().item()\n",
    "        # pred_max_value = F.softmax(predicted_masks).max().item()\n",
    "        # gt_min_value = obj_ground_truth_masks.min().item()\n",
    "        # gt_max_value = obj_ground_truth_masks.max().item()\n",
    "\n",
    "        # Print the min and max values for both predicted and ground truth masks\n",
    "        #print(f\"Predicted Mask - Min Value: {pred_min_value}, Max Value: {pred_max_value}\")\n",
    "        #print(f\"Ground Truth Mask - Min Value: {gt_min_value}, Max Value: {gt_max_value}\")\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    print(f'Mean loss: {mean(epoch_losses)}')\n",
    "     \n",
    "\n",
    "# Save the model's state dictionary to a file\n",
    "torch.save(medsam_model.state_dict(), \"Models/medsam_vit_b_object_masks.pth\")\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Loss Training Loop (Do not use while undersampling!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To define class weights I have used the average pixel frequency * class freq in an inverse ratio to the total. \n",
    "# I have capped the background as it has significantly more pixels than any object, and fillings as they were too common.\n",
    "\n",
    "# Define Class Weights\n",
    "class_weights = torch.tensor([\n",
    "    0.02,   # Background\n",
    "    0.133,  # braces\n",
    "    0.184,  # bridge\n",
    "    0.086,  # cavity\n",
    "    0.081,  # crown\n",
    "    0.05,   # filling\n",
    "    0.326,  # implant\n",
    "    0.172   # lesion \n",
    "    ]).to(device)  \n",
    "\n",
    "\n",
    "# Define the DiceCELoss\n",
    "dice_loss = monai.losses.DiceLoss(\n",
    "    sigmoid=True,            # sigmoid activation\n",
    "    squared_pred=True,       # square predictions for  dice calculation (peanalises false positives)\n",
    "    reduction=\"none\",        # how losses are aggregated (mean, sum, or none)\n",
    "    include_background = False\n",
    ")\n",
    "\n",
    "\n",
    "# Define the Focal Loss (good for imbalanced pixel coverage)\n",
    "focal_loss = monai.losses.FocalLoss(\n",
    "    gamma=2.0,             # Focusing (higher = more focus on hard examples)\n",
    "    reduction=\"none\",      # How losses are aggregated (mean, sum, or none)\n",
    "    include_background = False\n",
    ")\n",
    "\n",
    "# Define Tversky Loss (false pos not that imporant, but false neg is bad)\n",
    "tversky_loss = monai.losses.TverskyLoss(\n",
    "    alpha=0.7,              # increase for less false positives\n",
    "    beta=0.3,               # increase for less false negatives\n",
    "    reduction=\"none\",       # how losses are aggregated (mean, sum, or none)\n",
    "    include_background = False\n",
    ")\n",
    "\n",
    "\n",
    "# Maybe also try focal or dicefocal loss?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Training loop\n",
    "num_epochs = 1\n",
    "batch_losses = []  # Store loss for each batch\n",
    "\n",
    "medsam_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for batch in tqdm(train_dataloader):\n",
    "\n",
    "        \n",
    "\n",
    "        # print(batch.keys())\n",
    "\n",
    "\n",
    "        # Get batch values for inference\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        input_boxes = batch[\"input_boxes\"].to(device)\n",
    "        obj_ground_truth_masks = batch[\"obj_ground_truth_masks\"].float().to(device).squeeze(1)  # Remove extra singleton dimension\n",
    "\n",
    "         # To ge the mean of each individual image in the batch\n",
    "        batch_loss_values = []\n",
    "\n",
    "      \n",
    "        \n",
    "        # Loops through each image in the batch, removes the padding on input_boxes and obj_ground_truth_masks to ensure the loss isn't miscalculated by using empty inputs (because that happens??)\n",
    "        # We take the mean of the batch still (using batch_loss_values) to ensure a smoother gradient by avoiding the noise from using individual images\n",
    "        for image, input_box, obj_mask in zip(pixel_values, input_boxes, obj_ground_truth_masks):\n",
    "             \n",
    "            # Remove the padding from these batch values\n",
    "            input_box, obj_mask = remove_invalid_boxes(input_box, obj_mask)\n",
    "\n",
    "\n",
    "            # If the input somehow has no object masks we can skip\n",
    "            if input_box.shape[1] > 0:\n",
    "\n",
    "\n",
    "                # forward pass\n",
    "                outputs = medsam_model(\n",
    "                    pixel_values=image.unsqueeze(0), # Add batch dimension back\n",
    "                    input_boxes=input_box,\n",
    "                    multimask_output=False)\n",
    "\n",
    "\n",
    "\n",
    "                # Get predicted masks and ground truth masks\n",
    "                predicted_masks = outputs.pred_masks.squeeze(2)  # Remove extra singleton dimension from predicted masks (shape: [1, 20, 256, 256])\n",
    "            \n",
    "                # Convert object ground truth masks to binary to pass into MONAI loss function\n",
    "                obj_mask = (obj_mask > 0).float()\n",
    "\n",
    "\n",
    "                # Ensure the predicted and ground truth masks have the same shape\n",
    "                #print(\"\\n\\nPredicted Mask shape: \",predicted_masks.shape)\n",
    "                #print(\"obj gt shape: \", obj_mask.shape)\n",
    "\n",
    "                # Convert logits to probabilities\n",
    "                predicted_masks = torch.sigmoid(predicted_masks) \n",
    "\n",
    "                ground_truth_mask = batch[\"ground_truth_mask\"]\n",
    "\n",
    "\n",
    "                # Create a weight map by mapping each pixel’s ground truth to its corresponding weight\n",
    "                weight_map = torch.zeros_like(ground_truth_mask, dtype=torch.float).cuda()\n",
    "                for class_idx in range(len(class_weights)):  \n",
    "                    weight_map[ground_truth_mask == class_idx] = class_weights[class_idx]\n",
    "\n",
    "\n",
    "                # Calculate per-pixel weighted loss\n",
    "                loss_per_pixel = dice_loss(predicted_masks, obj_mask)\n",
    "\n",
    "                # Apply weight map to loss\n",
    "                weighted_loss = (loss_per_pixel * weight_map.unsqueeze(1)).mean()\n",
    "\n",
    "                # Add loss to batch list\n",
    "                batch_loss_values.append(weighted_loss)\n",
    "                print(\"Object Loss: \",weighted_loss)\n",
    "\n",
    "\n",
    "\n",
    "                # Show predictions\n",
    "                # with torch.no_grad():\n",
    "                #         first_pred_mask = torch.sigmoid(predicted_masks[0, 0]).cpu().numpy()  # Convert to numpy for plotting\n",
    "                #         first_gt_mask = obj_mask[0, 0].cpu().numpy()\n",
    "\n",
    "                #         fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "                #         ax[0].imshow(first_pred_mask, cmap=\"gray\")\n",
    "                #         ax[0].set_title(\"Predicted Mask\")\n",
    "\n",
    "                #         ax[1].imshow(first_gt_mask, cmap=\"gray\")\n",
    "                #         ax[1].set_title(\"Ground Truth Mask\")\n",
    "\n",
    "                #         plt.show()\n",
    "\n",
    "\n",
    "            else:\n",
    "                # Debug print to catch the missing masks\n",
    "                print(\"No masks found for:\", input_box)\n",
    "\n",
    "\n",
    "\n",
    "         # Calculate the mean of the batch and convert into a torch tensor for backpropagation\n",
    "        loss = torch.stack(batch_loss_values).mean() if batch_loss_values else torch.tensor(0.0)\n",
    "        print(\"Batch Loss: \", loss)\n",
    "\n",
    "        # backward pass\n",
    "        adam_optimizer.zero_grad()\n",
    "        weighted_loss.backward()\n",
    "\n",
    "        # optimize\n",
    "        adam_optimizer.step()\n",
    "        epoch_losses.append(weighted_loss.item())\n",
    "\n",
    "\n",
    "        # Store batch loss\n",
    "        batch_losses.append(weighted_loss.item())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # NOTE: sigmoid and softmax have the same output range here indicating that the SAM and MedSAM models only predict one class by default. This might \n",
    "        # Calculate the max and min values for both predicted and ground truth masks\n",
    "        # pred_min_value = F.softmax(predicted_masks).min().item()\n",
    "        # pred_max_value = F.softmax(predicted_masks).max().item()\n",
    "        # gt_min_value = obj_ground_truth_masks.min().item()\n",
    "        # gt_max_value = obj_ground_truth_masks.max().item()\n",
    "\n",
    "        # Print the min and max values for both predicted and ground truth masks\n",
    "        #print(f\"Predicted Mask - Min Value: {pred_min_value}, Max Value: {pred_max_value}\")\n",
    "        #print(f\"Ground Truth Mask - Min Value: {gt_min_value}, Max Value: {gt_max_value}\")\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    print(f'Mean loss: {mean(epoch_losses)}')\n",
    "     \n",
    "\n",
    "# Save the model's state dictionary to a file\n",
    "torch.save(medsam_model.state_dict(), \"Models/medsam_vit_b_object_masks.pth\")\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Training loop\n",
    "num_epochs = 1\n",
    "\n",
    "medsam_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for batch in tqdm(train_dataloader):\n",
    "\n",
    "        # forward pass\n",
    "        outputs = medsam_model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                        #input_masks=batch[\"labels\"].to(device),\n",
    "                        input_boxes=batch[\"input_boxes\"].to(device),\n",
    "                        multimask_output=False)\n",
    "\n",
    "\n",
    "        # Get predicted masks and ground truth masks\n",
    "        predicted_masks = outputs.pred_masks.squeeze(2)  # Remove extra singleton dimension from predicted masks (shape: [2, n, 256, 256])\n",
    "        ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device).squeeze(1)  # Remove extra singleton dimension from ground truth (shape: [2, n, 256, 256])\n",
    "\n",
    "       \n",
    "       \n",
    "\n",
    "\n",
    "    ################################### TESTING ########################################\n",
    "\n",
    "\n",
    "        # # Initialize list for binary masks per class\n",
    "        # num_classes = len(class_weights)  # Number of classes, including background\n",
    "        # binary_masks = []\n",
    "\n",
    "\n",
    "        # # Convert ground truth mask to binary mask for each class\n",
    "        # for class_idx in range(1, num_classes):  # Skip background (class 0)\n",
    "        #     binary_mask = (ground_truth_masks == class_idx).float()  # Binary mask for current class\n",
    "        #     binary_masks.append(binary_mask)\n",
    "\n",
    "\n",
    "        # # Stack the binary masks to match shape [batch_size, num_classes, height, width]\n",
    "        # binary_masks = torch.stack(binary_masks, dim=1)  # Shape: [batch_size, num_classes-1, height, width]\n",
    "\n",
    "        print(\"Min Class Label:\", ground_truth_masks.min().item())  \n",
    "        print(\"Max Class Label:\", ground_truth_masks.max().item())\n",
    "\n",
    "\n",
    "        # Reshape to merge the 'N' dimension into the batch\n",
    "        predicted_masks = predicted_masks.view(-1, 1, 256, 256)  # (2*35, 1, 256, 256)\n",
    "        ground_truth_masks = ground_truth_masks.view(-1, 1, 256, 256)  # (2*35, 1, 256, 256)\n",
    "\n",
    "        # Convert ground truth to one-hot encoding for multi-class segmentation\n",
    "        ground_truth_masks = F.one_hot(ground_truth_masks.squeeze(1).long(), num_classes=8).permute(0, 3, 1, 2).float()\n",
    "\n",
    "        # Ensure predictions have the correct shape (B, C, H, W)\n",
    "        predicted_masks = predicted_masks.repeat(1, 8, 1, 1)  # If needed, duplicate across 8 channels\n",
    "\n",
    "        # Compute loss\n",
    "        #loss = dice_ce_loss(predicted_masks, ground_truth_masks)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ################################### TESTING ########################################\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "        # Calculate loss using defined loss function\n",
    "        loss = dice_ce_loss(predicted_masks, ground_truth_masks)\n",
    "        print(loss)\n",
    "\n",
    "\n",
    "        # backward pass\n",
    "        adam_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # optimize\n",
    "        adam_optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # # NOTE: sigmoid and softmax have the same output range here indicating that the SAM and MedSAM models only predict one class by default. This might \n",
    "        # # Calculate the max and min values for both predicted and ground truth masks\n",
    "        # pred_min_value = F.softmax(predicted_masks).min().item()\n",
    "        # pred_max_value = F.softmax(predicted_masks).max().item()\n",
    "        # gt_min_value = ground_truth_masks.min().item()\n",
    "        # gt_max_value = ground_truth_masks.max().item()\n",
    "\n",
    "        # Print the min and max values for both predicted and ground truth masks\n",
    "        #print(f\"Predicted Mask - Min Value: {pred_min_value}, Max Value: {pred_max_value}\")\n",
    "        #print(f\"Ground Truth Mask - Min Value: {gt_min_value}, Max Value: {gt_max_value}\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    print(f'Mean loss: {mean(epoch_losses)}')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Save the model's state dictionary to a file\n",
    "torch.save(medsam_model.state_dict(), \"Models/medsam_vit_b_object_masks.pth\")\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute moving average (window size = 10)\n",
    "window_size = 10\n",
    "smoothed_losses = np.convolve(batch_losses, np.ones(window_size) / window_size, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(batch_losses, alpha=0.3, label=\"Raw Loss\")  # Light color for raw loss\n",
    "plt.plot(range(window_size - 1, len(batch_losses)), smoothed_losses, color='red', label=\"Smoothed Loss\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss per Batch (Smoothed)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuned MedSAM Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model configuration\n",
    "model_config = SamConfig.from_pretrained(\"flaviagiammarino/medsam-vit-base\")\n",
    "processor = SamProcessor.from_pretrained(\"flaviagiammarino/medsam-vit-base\")\n",
    "\n",
    "# Create an instance of the model architecture with the loaded configuration\n",
    "my_model = SamModel(config=model_config)\n",
    "#Update the model by loading the weights from saved file.\n",
    "my_model.load_state_dict(torch.load(\"Models\\medsam_vit_b_object_masks_test.pth\")) \n",
    "\n",
    "# set the device to cuda\n",
    "device = \"cuda\"\n",
    "my_model.to(device)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "# Get image from batch\n",
    "image = batch[\"pixel_values\"][0].detach().cpu().numpy().transpose(1, 2, 0)  # Convert to HxWxC format\n",
    "\n",
    "# Get ground truth mask\n",
    "ground_truth = batch[\"obj_ground_truth_masks\"][0][0].detach().cpu().numpy()  # Convert to numpy (H, W)\n",
    "\n",
    "# Get model prediction\n",
    "image_tensor = batch[\"pixel_values\"][0].unsqueeze(0).to(device)  # Add batch dimension\n",
    "input_boxes = batch[\"input_boxes\"][0].unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = my_model(pixel_values=image_tensor, input_boxes=input_boxes, multimask_output=False)\n",
    "    predicted_mask = torch.sigmoid(outputs.pred_masks.squeeze(2)).cpu().numpy()[0]  # Convert to numpy (H, W)\n",
    "\n",
    "# Plot the results\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "# Display input image\n",
    "ax1.imshow(image)\n",
    "for box in batch[\"input_boxes\"][0]:\n",
    "    rect = patches.Rectangle(\n",
    "        (box[0], box[1]), \n",
    "        box[2] - box[0],  \n",
    "        box[3] - box[1],  \n",
    "        linewidth=2, \n",
    "        edgecolor='red', \n",
    "        facecolor='none'\n",
    "    )\n",
    "    ax1.add_patch(rect)\n",
    "\n",
    "ax1.set_title(\"Example Input\")\n",
    "\n",
    "# Display ground truth mask\n",
    "ax2.imshow(ground_truth, cmap='gray')\n",
    "\n",
    "# Plot boxes for the second image\n",
    "box = batch[\"input_boxes\"][0][0]\n",
    "box = (box / torch.tensor([1024,1024,1024,1024], device=\"cpu\")) * 256\n",
    "\n",
    "rect = patches.Rectangle(\n",
    "    (box[0], box[1]),  \n",
    "    box[2] - box[0],  \n",
    "    box[3] - box[1],  \n",
    "    linewidth=2, \n",
    "    edgecolor='green', \n",
    "    facecolor='none'\n",
    ")\n",
    "ax2.add_patch(rect)\n",
    "ax2.set_title(\"Example GT (Loss)\")\n",
    "\n",
    "# Threshold prediction to get a clearer segmentation\n",
    "threshold_prediction = (predicted_mask[0] > 0.7).astype(np.uint8)\n",
    "\n",
    "# Display predicted mask\n",
    "ax3.imshow(threshold_prediction, cmap=\"viridis\")\n",
    "ax3.set_title(\"Predicted Mask\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not working old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importlib.reload(model_evaluator)\n",
    "evaluator = ModelEvaluator(my_model, processor, test_dataset)\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Get correct preprocessing\n",
    "test_dataset.return_as_medsam = True\n",
    "test_dataset.resize_mask = False\n",
    "\n",
    "# Load random image\n",
    "#image_idx = random.randint(0, len(test_dataset)-1)\n",
    "\n",
    "image_idx = 33\n",
    "\n",
    "# Get tensors\n",
    "img_np, box_np, gt_masks, bounding_boxes = test_dataset[image_idx].values()\n",
    "\n",
    "# Get original image\n",
    "test_dataset.return_as_medsam = False\n",
    "img_original = test_dataset[image_idx][\"pixel_values\"]\n",
    "W, H, _ = img_original.shape\n",
    "\n",
    "# Show image\n",
    "test_dataset.show_image_mask(image_idx)\n",
    "\n",
    "# image embedding\n",
    "with torch.no_grad():\n",
    "    image_embedding = my_model.image_encoder(img_np)\n",
    "\n",
    "\n",
    "# Run inference for all boxes in a batch\n",
    "with torch.no_grad():\n",
    "    seg_masks = evaluator.medsam_inference(image_embedding, box_np, H, W)  # List of 5 masks\n",
    "\n",
    "# Plot results\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "\n",
    "if len(seg_masks.shape) == 2:\n",
    "    seg_masks = [seg_masks]\n",
    "\n",
    "\n",
    "# Original image with bounding boxes\n",
    "ax[0].imshow(img_original)\n",
    "for box in bounding_boxes:\n",
    "    show_box(box, ax[0])\n",
    "ax[0].set_title(\"Input Image and Bounding Boxes\")\n",
    "\n",
    "# Image with segmentation masks\n",
    "ax[1].imshow(img_original)\n",
    "for box, mask in zip(bounding_boxes, seg_masks):  # Iterate over all boxes and their masks\n",
    "    show_mask(mask, ax[1]) # random_colour = True\n",
    "    show_box(box, ax[1])\n",
    "ax[1].set_title(\"Base MedSAM Segmentation\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify batch item sizes\n",
    "batch = next(iter(train_dataloader))\n",
    "for k,v in batch.items():\n",
    "  print(f\"{k:<25} Shape: {str(v.shape):<30} Dtype: {v.dtype}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Example Input:\\n\")\n",
    "\n",
    "# Get image from batch\n",
    "image = batch[\"pixel_values\"][0].detach().cpu().numpy().transpose(1, 2, 0)  # Convert to HxWxC format\n",
    "\n",
    "print(image.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.imshow(image)  # Show the image\n",
    "\n",
    "# Plot predicted boxes\n",
    "for box in batch[\"input_boxes\"][0]:\n",
    "    rect = patches.Rectangle(\n",
    "        (box[0], box[1]),  # x, y (top-left corner)\n",
    "        box[2] - box[0],  # width\n",
    "        box[3] - box[1],  # height\n",
    "        linewidth=2,\n",
    "        edgecolor='red',\n",
    "        facecolor='none',\n",
    "        label='Predicted Box'\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "ax.set_title(f\"Test Inference\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MedSAM Fine-Tuned Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.preprocess_for_fine_tuning  = True\n",
    "test_dataset.resize_mask  = True\n",
    "test_dataset.return_individual_objects = True\n",
    "\n",
    "\n",
    "\n",
    "importlib.reload(model_evaluator)\n",
    "evaluator = ModelEvaluator(my_model, processor, test_dataset)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,  batch_size=1, shuffle=False)\n",
    "\n",
    "results = evaluator.evaluate_medsam_model(test_dataloader)\n",
    "evaluator.print_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
